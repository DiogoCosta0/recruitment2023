{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Implementing and training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment verification\n",
    "Start by confirming you have PyTorch, TorchVision and TensorBoard installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T19:18:17.153761956Z",
     "start_time": "2023-10-07T19:18:14.163459607Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "The used dataset is the well-known MNIST, which is composed of images of handwritten digits with 28 pixels wide and 28 pixels high.\n",
    "\n",
    "The goals of most of the models using this dataset is to classify the digit of the image, which is our case.\n",
    "\n",
    "Download the training and validation dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "training_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "validation_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:18:17.316292073Z",
     "start_time": "2023-10-07T19:18:17.148000901Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - MLP evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the example MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T19:18:17.377277186Z",
     "start_time": "2023-10-07T19:18:17.268839588Z"
    }
   },
   "outputs": [],
   "source": [
    "from bobnet import BobNet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an instance of this model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model1 = BobNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:18:17.399111060Z",
     "start_time": "2023-10-07T19:18:17.273094741Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the hyperparameters for this model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# batch size\n",
    "MLP_BATCH_SIZE=64\n",
    "\n",
    "# learning rate\n",
    "MLP_LEARNING_RATE=0.001\n",
    "\n",
    "# momentum\n",
    "MLP_MOMENTUM=0.9\n",
    "\n",
    "# training epochs to run\n",
    "MLP_EPOCHS=10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:18:17.443136058Z",
     "start_time": "2023-10-07T19:18:17.293604128Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the training and validation dataloaders from the datasets downloaded earlier:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create the training loader\n",
    "mlp_training_loader = DataLoader(training_set, batch_size=MLP_BATCH_SIZE, shuffle=True) \n",
    "\n",
    "# create the validation loader\n",
    "mlp_validation_loader = DataLoader(validation_set, batch_size=MLP_BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:18:17.459903734Z",
     "start_time": "2023-10-07T19:18:17.339770999Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the loss function and the optimizer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "mlp_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_optimizer = torch.optim.SGD(model1.parameters(), lr=MLP_LEARNING_RATE, momentum=MLP_MOMENTUM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:18:17.643325571Z",
     "start_time": "2023-10-07T19:18:17.340180929Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the training and validation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (99/938): training_loss = 2.3245476737166895\n",
      "Epoch 0 (199/938): training_loss = 2.311334980193095\n",
      "Epoch 0 (299/938): training_loss = 2.3059090244331486\n",
      "Epoch 0 (399/938): training_loss = 2.3022876287761487\n",
      "Epoch 0 (499/938): training_loss = 2.299366777072211\n",
      "Epoch 0 (599/938): training_loss = 2.2966790517701927\n",
      "Epoch 0 (699/938): training_loss = 2.2939833336122044\n",
      "Epoch 0 (799/938): training_loss = 2.2911752616061136\n",
      "Epoch 0 (899/938): training_loss = 2.288015873466636\n",
      "Epoch 0 (99/157): validation_loss = 2.273643732070923\n",
      "Epoch 1 (99/938): training_loss = 2.2649861995619958\n",
      "Epoch 1 (199/938): training_loss = 2.2487360820099336\n",
      "Epoch 1 (299/938): training_loss = 2.2371010915890186\n",
      "Epoch 1 (399/938): training_loss = 2.227154220853533\n",
      "Epoch 1 (499/938): training_loss = 2.2175696302272514\n",
      "Epoch 1 (599/938): training_loss = 2.2068267899482996\n",
      "Epoch 1 (699/938): training_loss = 2.19608653938992\n",
      "Epoch 1 (799/938): training_loss = 2.183740225840868\n",
      "Epoch 1 (899/938): training_loss = 2.1701834595640985\n",
      "Epoch 1 (99/157): validation_loss = 2.061938762664795\n",
      "Epoch 2 (99/938): training_loss = 2.0558886371477687\n",
      "Epoch 2 (199/938): training_loss = 2.035803550451844\n",
      "Epoch 2 (299/938): training_loss = 2.01910098460207\n",
      "Epoch 2 (399/938): training_loss = 2.009036581916618\n",
      "Epoch 2 (499/938): training_loss = 1.9993254849332607\n",
      "Epoch 2 (599/938): training_loss = 1.9885666030078182\n",
      "Epoch 2 (699/938): training_loss = 1.9784141657860665\n",
      "Epoch 2 (799/938): training_loss = 1.969660064604166\n",
      "Epoch 2 (899/938): training_loss = 1.9608038003505668\n",
      "Epoch 2 (99/157): validation_loss = 1.8970882892608643\n",
      "Epoch 3 (99/938): training_loss = 1.8950524257891106\n",
      "Epoch 3 (199/938): training_loss = 1.8734187588619826\n",
      "Epoch 3 (299/938): training_loss = 1.863214626360099\n",
      "Epoch 3 (399/938): training_loss = 1.8551743341269051\n",
      "Epoch 3 (499/938): training_loss = 1.8496345417771884\n",
      "Epoch 3 (599/938): training_loss = 1.844511458989177\n",
      "Epoch 3 (699/938): training_loss = 1.838954622141793\n",
      "Epoch 3 (799/938): training_loss = 1.8347111570372598\n",
      "Epoch 3 (899/938): training_loss = 1.8304510462668635\n",
      "Epoch 3 (99/157): validation_loss = 1.7991942167282104\n",
      "Epoch 4 (99/938): training_loss = 1.7953462684997405\n",
      "Epoch 4 (199/938): training_loss = 1.7872906887351567\n",
      "Epoch 4 (299/938): training_loss = 1.7808125609139536\n",
      "Epoch 4 (399/938): training_loss = 1.7740926509512995\n",
      "Epoch 4 (499/938): training_loss = 1.7710117104058276\n",
      "Epoch 4 (599/938): training_loss = 1.7677405654687515\n",
      "Epoch 4 (699/938): training_loss = 1.7636116045568464\n",
      "Epoch 4 (799/938): training_loss = 1.7603311819188736\n",
      "Epoch 4 (899/938): training_loss = 1.7577616614149727\n",
      "Epoch 4 (99/157): validation_loss = 1.7376374006271362\n",
      "Epoch 5 (99/938): training_loss = 1.7406480035396537\n",
      "Epoch 5 (199/938): training_loss = 1.7356133868346861\n",
      "Epoch 5 (299/938): training_loss = 1.7321975597968469\n",
      "Epoch 5 (399/938): training_loss = 1.7279546213030517\n",
      "Epoch 5 (499/938): training_loss = 1.7255434132291225\n",
      "Epoch 5 (599/938): training_loss = 1.7231751793812033\n",
      "Epoch 5 (699/938): training_loss = 1.7199818714493846\n",
      "Epoch 5 (799/938): training_loss = 1.717836092798522\n",
      "Epoch 5 (899/938): training_loss = 1.7166626893638637\n",
      "Epoch 5 (99/157): validation_loss = 1.7100119590759277\n",
      "Epoch 6 (99/938): training_loss = 1.7191361056433783\n",
      "Epoch 6 (199/938): training_loss = 1.7081973768358854\n",
      "Epoch 6 (299/938): training_loss = 1.7027345827989355\n",
      "Epoch 6 (399/938): training_loss = 1.69888294639444\n",
      "Epoch 6 (499/938): training_loss = 1.697410984603102\n",
      "Epoch 6 (599/938): training_loss = 1.695787436377027\n",
      "Epoch 6 (699/938): training_loss = 1.6944858553754072\n",
      "Epoch 6 (799/938): training_loss = 1.6943349794989384\n",
      "Epoch 6 (899/938): training_loss = 1.6938954431302555\n",
      "Epoch 6 (99/157): validation_loss = 1.6872365474700928\n",
      "Epoch 7 (99/938): training_loss = 1.7010596388518209\n",
      "Epoch 7 (199/938): training_loss = 1.691713437363131\n",
      "Epoch 7 (299/938): training_loss = 1.6881168454785809\n",
      "Epoch 7 (399/938): training_loss = 1.68573052124272\n",
      "Epoch 7 (499/938): training_loss = 1.6850416593417854\n",
      "Epoch 7 (599/938): training_loss = 1.683857804745784\n",
      "Epoch 7 (699/938): training_loss = 1.6811380966197438\n",
      "Epoch 7 (799/938): training_loss = 1.6811727762819082\n",
      "Epoch 7 (899/938): training_loss = 1.6801199997890248\n",
      "Epoch 7 (99/157): validation_loss = 1.6804332733154297\n",
      "Epoch 8 (99/938): training_loss = 1.6943704419665866\n",
      "Epoch 8 (199/938): training_loss = 1.679612309489418\n",
      "Epoch 8 (299/938): training_loss = 1.6760245557612798\n",
      "Epoch 8 (399/938): training_loss = 1.672666350701698\n",
      "Epoch 8 (499/938): training_loss = 1.672396774760229\n",
      "Epoch 8 (599/938): training_loss = 1.6723274420418206\n",
      "Epoch 8 (699/938): training_loss = 1.671290287473512\n",
      "Epoch 8 (799/938): training_loss = 1.6700429993964854\n",
      "Epoch 8 (899/938): training_loss = 1.6704680269896919\n",
      "Epoch 8 (99/157): validation_loss = 1.6755162477493286\n",
      "Epoch 9 (99/938): training_loss = 1.6846361750304097\n",
      "Epoch 9 (199/938): training_loss = 1.6770134816816704\n",
      "Epoch 9 (299/938): training_loss = 1.6725511570837983\n",
      "Epoch 9 (399/938): training_loss = 1.6692061920213819\n",
      "Epoch 9 (499/938): training_loss = 1.6668190796055153\n",
      "Epoch 9 (599/938): training_loss = 1.6656475321876385\n",
      "Epoch 9 (699/938): training_loss = 1.6652874208144706\n",
      "Epoch 9 (799/938): training_loss = 1.663761912955808\n",
      "Epoch 9 (899/938): training_loss = 1.6628985833273051\n",
      "Epoch 9 (99/157): validation_loss = 1.666296124458313\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(1.6506, device='cuda:0')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# how many batches between logs\n",
    "LOGGING_INTERVAL=100\n",
    "\n",
    "utils.train_model(model1, MLP_EPOCHS, mlp_optimizer, mlp_loss_fn, mlp_training_loader, mlp_validation_loader, LOGGING_INTERVAL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:20:45.041048475Z",
     "start_time": "2023-10-07T19:18:17.340477874Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "Explore the architecture on the script `mod1/bobnet.py`.\n",
    "1. Why does the input layer have 784 input features?\n",
    "2. Why does the output layer have 10 output features?\n",
    "3. What would happen if the dataset had a ratio of 100 samples of the number 7 to 1 sample of the number 1?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - CNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Head over to the `cnn.py` file and implement a convolutional architecture capable of surpassing the MLP results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, import the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from cnn import CNN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:20:45.044467404Z",
     "start_time": "2023-10-07T19:20:45.025606909Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an instance of this model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Define the layers here!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model2 \u001B[38;5;241m=\u001B[39m \u001B[43mCNN\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Formula/recruitment2023/mod1/cnn.py:9\u001B[0m, in \u001B[0;36mCNN.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m----> 9\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDefine the layers here!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: Define the layers here!"
     ]
    }
   ],
   "source": [
    "model2 = CNN()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:20:45.449194023Z",
     "start_time": "2023-10-07T19:20:45.031103652Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: run training"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-07T19:20:45.438977058Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. The tanh and softmax activation functions are differentiable. Can you explain why not using non-differentiable functions?\n",
    "2. What changed in the results when comparing with the MLP? Do you have any guess why?\n",
    "3. What results would you expect if you used an attention mechanism like CBAM (Convolutional Block Attention Module)? What do these mechanisms do?\n",
    "4. Why does the MLP implementation start with a `torch.nn.Flatten` layer? Was it needed in the CNN?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
