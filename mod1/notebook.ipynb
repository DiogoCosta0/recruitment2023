{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Implementing and training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment verification\n",
    "Start by confirming you have PyTorch, TorchVision and TensorBoard installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T10:33:12.013848031Z",
     "start_time": "2023-10-09T10:33:09.553539200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General autonomous driving questions\n",
    "1. List some pros and cons of using a stereo camera versus LiDAR versus RADAR for perception. You can research examples from the industry on why do they use specific sensors and not others. \n",
    "2. Stereo cameras are capable of perceiving both color and depth for each pixel. These cameras can be bought plug-and-play solutions (for example Intel RealSense or StereoLabs ZED 2) or self-made using industrial cameras (for example Basler). Computing depth from multiple cameras requires processing, called \"depth estimation\", which is done onboard on the plug and play solutions. Which solution which you opt for if you had a small team with a short budget? Consider complexity, reliability and cost on your decision.\n",
    "3. In an autonomous car, monitorization and reaction to critical failures are essential to prevent uncontrolled behavior. According to the rulebook and the beginner's guide, what must happen if the car detects a camera and/or LiDAR malfunction? Select the correct option(s):\n",
    "    1. Play a sound using the TSAC.\n",
    "    2. Eject the processing computer.\n",
    "    3. Activate the EBS.\n",
    "    4. Send a text message to the officials notifying the issue.\n",
    "    5. Autonomously approach the ASR to perform a safe shutdown.\n",
    "4. Usually an autonomous driving pipeline is divided into perception, planning and control. Which algorithms are most commonly used by formula student teams on each of these stages? You can research other teams' social media or FSG Academy, for example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "The used dataset is the well-known MNIST, which is composed of images of handwritten digits (0 to 9) with 28 pixels wide and 28 pixels high.\n",
    "\n",
    "The goals of most of the models using this dataset is to classify the digit of the image, which is our case.\n",
    "\n",
    "Download the training and validation dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "training_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "validation_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:33:12.102629994Z",
     "start_time": "2023-10-09T10:33:12.050995197Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - MLP evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the example MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T10:33:12.107102694Z",
     "start_time": "2023-10-09T10:33:12.104520401Z"
    }
   },
   "outputs": [],
   "source": [
    "from bobnet import BobNet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an instance of this model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model1 = BobNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:33:12.117744590Z",
     "start_time": "2023-10-09T10:33:12.110307045Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the hyperparameters for this model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# batch size\n",
    "MLP_BATCH_SIZE=64\n",
    "\n",
    "# learning rate\n",
    "MLP_LEARNING_RATE=0.001\n",
    "\n",
    "# momentum\n",
    "MLP_MOMENTUM=0.9\n",
    "\n",
    "# training epochs to run\n",
    "MLP_EPOCHS=10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:33:12.122642108Z",
     "start_time": "2023-10-09T10:33:12.120840283Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the training and validation dataloaders from the datasets downloaded earlier:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create the training loader\n",
    "mlp_training_loader = DataLoader(training_set, batch_size=MLP_BATCH_SIZE, shuffle=True) \n",
    "\n",
    "# create the validation loader\n",
    "mlp_validation_loader = DataLoader(validation_set, batch_size=MLP_BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:33:12.167193670Z",
     "start_time": "2023-10-09T10:33:12.125098869Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the loss function and the optimizer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "mlp_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_optimizer = torch.optim.SGD(model1.parameters(), lr=MLP_LEARNING_RATE, momentum=MLP_MOMENTUM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:33:12.167473235Z",
     "start_time": "2023-10-09T10:33:12.167071028Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the training and validation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlostojal/Documents/Formula/recruitment2023/venv/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (99/938): training_loss = 2.3243640817777074\n",
      "Epoch 0 (199/938): training_loss = 2.3111291051509992\n",
      "Epoch 0 (299/938): training_loss = 2.3055640024485\n",
      "Epoch 0 (399/938): training_loss = 2.3018544783867094\n",
      "Epoch 0 (499/938): training_loss = 2.298740706606236\n",
      "Epoch 0 (599/938): training_loss = 2.295521767589206\n",
      "Epoch 0 (699/938): training_loss = 2.292215309429578\n",
      "Epoch 0 (799/938): training_loss = 2.28838352088785\n",
      "Epoch 0 (899/938): training_loss = 2.2841784943462877\n",
      "Epoch 0 (99/157): validation_loss = 2.263949394226074\n",
      "Epoch 1 (99/938): training_loss = 2.257587052354909\n",
      "Epoch 1 (199/938): training_loss = 2.2422685359590617\n",
      "Epoch 1 (299/938): training_loss = 2.233656132101614\n",
      "Epoch 1 (399/938): training_loss = 2.224944743894993\n",
      "Epoch 1 (499/938): training_loss = 2.2170839883043674\n",
      "Epoch 1 (599/938): training_loss = 2.2093011556762288\n",
      "Epoch 1 (699/938): training_loss = 2.2004304078856594\n",
      "Epoch 1 (799/938): training_loss = 2.190313891266404\n",
      "Epoch 1 (899/938): training_loss = 2.17940024393419\n",
      "Epoch 1 (99/157): validation_loss = 2.0928657054901123\n",
      "Epoch 2 (99/938): training_loss = 2.0771061784089215\n",
      "Epoch 2 (199/938): training_loss = 2.0540943115799872\n",
      "Epoch 2 (299/938): training_loss = 2.0396492752343116\n",
      "Epoch 2 (399/938): training_loss = 2.0259214538081847\n",
      "Epoch 2 (499/938): training_loss = 2.01558158631793\n",
      "Epoch 2 (599/938): training_loss = 2.0053743424519075\n",
      "Epoch 2 (699/938): training_loss = 1.994979587235676\n",
      "Epoch 2 (799/938): training_loss = 1.9856101735512515\n",
      "Epoch 2 (899/938): training_loss = 1.9775166756849534\n",
      "Epoch 2 (99/157): validation_loss = 1.9115599393844604\n",
      "Epoch 3 (99/938): training_loss = 1.912316114011437\n",
      "Epoch 3 (199/938): training_loss = 1.8970439152501934\n",
      "Epoch 3 (299/938): training_loss = 1.8857051241756682\n",
      "Epoch 3 (399/938): training_loss = 1.8816778492509274\n",
      "Epoch 3 (499/938): training_loss = 1.8749782059617892\n",
      "Epoch 3 (599/938): training_loss = 1.867899088111266\n",
      "Epoch 3 (699/938): training_loss = 1.861366912530727\n",
      "Epoch 3 (799/938): training_loss = 1.8552956527404403\n",
      "Epoch 3 (899/938): training_loss = 1.8496796273283487\n",
      "Epoch 3 (99/157): validation_loss = 1.8060680627822876\n",
      "Epoch 4 (99/938): training_loss = 1.8102385540201207\n",
      "Epoch 4 (199/938): training_loss = 1.7917077954690062\n",
      "Epoch 4 (299/938): training_loss = 1.7854597815701794\n",
      "Epoch 4 (399/938): training_loss = 1.779971136485126\n",
      "Epoch 4 (499/938): training_loss = 1.776044721832734\n",
      "Epoch 4 (599/938): training_loss = 1.772484533774833\n",
      "Epoch 4 (699/938): training_loss = 1.7689573295808827\n",
      "Epoch 4 (799/938): training_loss = 1.765699038368292\n",
      "Epoch 4 (899/938): training_loss = 1.7623837661159716\n",
      "Epoch 4 (99/157): validation_loss = 1.7443166971206665\n",
      "Epoch 5 (99/938): training_loss = 1.754103998945217\n",
      "Epoch 5 (199/938): training_loss = 1.7374661687630504\n",
      "Epoch 5 (299/938): training_loss = 1.7338544405423677\n",
      "Epoch 5 (399/938): training_loss = 1.7295849616067451\n",
      "Epoch 5 (499/938): training_loss = 1.7268430622880588\n",
      "Epoch 5 (599/938): training_loss = 1.724847443314745\n",
      "Epoch 5 (699/938): training_loss = 1.7228236655479507\n",
      "Epoch 5 (799/938): training_loss = 1.7203939265393196\n",
      "Epoch 5 (899/938): training_loss = 1.719116050754161\n",
      "Epoch 5 (99/157): validation_loss = 1.7131556272506714\n",
      "Epoch 6 (99/938): training_loss = 1.7171263791093923\n",
      "Epoch 6 (199/938): training_loss = 1.7093576636146661\n",
      "Epoch 6 (299/938): training_loss = 1.7068080702752972\n",
      "Epoch 6 (399/938): training_loss = 1.704341821801991\n",
      "Epoch 6 (499/938): training_loss = 1.7027917811292446\n",
      "Epoch 6 (599/938): training_loss = 1.7010056094852632\n",
      "Epoch 6 (699/938): training_loss = 1.699495506218404\n",
      "Epoch 6 (799/938): training_loss = 1.6979754059126737\n",
      "Epoch 6 (899/938): training_loss = 1.695812337523175\n",
      "Epoch 6 (99/157): validation_loss = 1.6903502941131592\n",
      "Epoch 7 (99/938): training_loss = 1.705941368835141\n",
      "Epoch 7 (199/938): training_loss = 1.6950193859224942\n",
      "Epoch 7 (299/938): training_loss = 1.6887566289774152\n",
      "Epoch 7 (399/938): training_loss = 1.6867785949754834\n",
      "Epoch 7 (499/938): training_loss = 1.684662119659011\n",
      "Epoch 7 (599/938): training_loss = 1.6837970770659152\n",
      "Epoch 7 (699/938): training_loss = 1.68272149648107\n",
      "Epoch 7 (799/938): training_loss = 1.6813869477810341\n",
      "Epoch 7 (899/938): training_loss = 1.6803774476979545\n",
      "Epoch 7 (99/157): validation_loss = 1.6827038526535034\n",
      "Epoch 8 (99/938): training_loss = 1.6845911548595236\n",
      "Epoch 8 (199/938): training_loss = 1.6792003952678125\n",
      "Epoch 8 (299/938): training_loss = 1.6779727234090849\n",
      "Epoch 8 (399/938): training_loss = 1.675356261712268\n",
      "Epoch 8 (499/938): training_loss = 1.6735324713892354\n",
      "Epoch 8 (599/938): training_loss = 1.6727747342025299\n",
      "Epoch 8 (699/938): training_loss = 1.6727967777306771\n",
      "Epoch 8 (799/938): training_loss = 1.671191617454844\n",
      "Epoch 8 (899/938): training_loss = 1.6708219662656774\n",
      "Epoch 8 (99/157): validation_loss = 1.6734603643417358\n",
      "Epoch 9 (99/938): training_loss = 1.6831706437197598\n",
      "Epoch 9 (199/938): training_loss = 1.6784883306254095\n",
      "Epoch 9 (299/938): training_loss = 1.6733996290028295\n",
      "Epoch 9 (399/938): training_loss = 1.6712888933363415\n",
      "Epoch 9 (499/938): training_loss = 1.6694819549759308\n",
      "Epoch 9 (599/938): training_loss = 1.6677331309485715\n",
      "Epoch 9 (699/938): training_loss = 1.6657083628003688\n",
      "Epoch 9 (799/938): training_loss = 1.66505437753675\n",
      "Epoch 9 (899/938): training_loss = 1.663797055521319\n",
      "Epoch 9 (99/157): validation_loss = 1.668143391609192\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(1.6517)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# how many batches between logs\n",
    "LOGGING_INTERVAL=100\n",
    "\n",
    "utils.train_model(model1, MLP_EPOCHS, mlp_optimizer, mlp_loss_fn, mlp_training_loader, mlp_validation_loader, LOGGING_INTERVAL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:34:37.351812976Z",
     "start_time": "2023-10-09T10:33:12.167361328Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "Explore the architecture on the script `mod1/bobnet.py`.\n",
    "1. Why does the input layer have 784 inputs? Consider the MNIST dataset samples' characteristics.\n",
    "2. Why does the output layer have 10 outputs?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - CNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Head over to the `cnn.py` file and implement a convolutional architecture (add some convolutional layers and fully connected layers). You can search the LeNet architecture or AlexNet to get some insights and/or inspiration (you can implement a simpler version: with less layers). 2D convolutional layers in PyTorch are created using the `torch.nn.Conv2d` class. Activation and loss functions can be found under `torch.nn.functional` (like ReLU and softmax)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, import the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from cnn import CNN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:34:37.360031821Z",
     "start_time": "2023-10-09T10:34:37.354897323Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an instance of this model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Define the layers here!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model2 \u001B[38;5;241m=\u001B[39m \u001B[43mCNN\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Formula/recruitment2023/mod1/cnn.py:9\u001B[0m, in \u001B[0;36mCNN.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m----> 9\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDefine the layers here!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: Define the layers here!"
     ]
    }
   ],
   "source": [
    "model2 = CNN()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:34:37.650467496Z",
     "start_time": "2023-10-09T10:34:37.359444877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: run training and validation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T10:34:37.657014666Z",
     "start_time": "2023-10-09T10:34:37.653936659Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. What are the advantages of using convolutional layers versus fully-connected layers for image processing?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
